{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考  \n",
    "aidiaryさんのブログ : http://aidiary.hatenablog.com/entry/20180129/1517233796  \n",
    "pytorch tutorial : http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable classにあるbackwardメソッドを呼ぶことで導関数を求めることができる．  \n",
    "Variableでrequires_grad=Trueにすると導関数を計算することができる．  \n",
    "requires_grad=FalseだとgradでNoneを返す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 3\n",
      " 3\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=Variable(torch.ones(2,2), requires_grad=True)\n",
    "w=torch.ones(2,1)\n",
    "w.fill_(3.0)\n",
    "w=Variable(w, requires_grad=True)\n",
    "print(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y=x+2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x1116c4a58>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=y*y*3\n",
    "out=z.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nx = Variable(torch.Tensor([2]), requires_grad=True)\\nx = np.exp(x)\\nprint(type(x))\\ny = torch.from_numpy(x)\\ny.backward()\\nprint(x.grad)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "x = np.exp(x)\n",
    "print(type(x))\n",
    "y = torch.from_numpy(x)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpyに変換する関数があるけど上記のようにexpとかsinとかを変換するのには使えない  \n",
    "だからtorchの中にある関数でやる必要がある  \n",
    "上記のやつは下記のようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 7.3891\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "y = torch.exp(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sin,cos,squrなども同様  \n",
    "- sinの微分\n",
    "$$\n",
    "y = \\sin(x) \\\\\n",
    "\\frac{dy}{dx} = \\cos(x)\n",
    "$$\n",
    "\n",
    "- cosの微分\n",
    "$$\n",
    "y = \\cos(x)\\\\\n",
    "\\frac{dy}{dx}=-\\sin(x)\n",
    "$$\n",
    "\n",
    "- sqrtの微分\n",
    "$$\n",
    "y=\\sqrt{x}\\\\\n",
    "\\frac{dy}{dx}=\\frac{1}{2\\sqrt{x}}\n",
    "$$\n",
    "\n",
    "- expの微分\n",
    "$$\n",
    "y=\\mathrm{e}^{x}\\\\\n",
    "\\frac{dy}{dx} = \\mathrm{e}^{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.4161\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      "-0.4161\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-0.9093\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      "-0.9093\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.3536\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.3536\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 7.3891\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 7.3891\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sin\n",
    "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "y = torch.sin(x)\n",
    "z = torch.cos(x)\n",
    "y.backward()\n",
    "print(x.grad, z)\n",
    "\n",
    "#cos\n",
    "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "y = torch.cos(x)\n",
    "z = torch.sin(-x)\n",
    "y.backward()\n",
    "print(x.grad, z)\n",
    "\n",
    "#sqrt\n",
    "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "y = torch.sqrt(x)\n",
    "z = 1/(2*torch.sqrt(x))\n",
    "y.backward()\n",
    "print(x.grad, z)\n",
    "\n",
    "#exp\n",
    "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "y = torch.exp(x)\n",
    "y.backward()\n",
    "print(x.grad, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss(二乗誤差)を微分する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: Parameter containing:\n",
      "-0.4080  0.1045  0.0191\n",
      " 0.4515  0.4651 -0.4163\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "b: Parameter containing:\n",
      " 0.2342\n",
      " 0.2078\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "loss: Variable containing:\n",
      " 0.8999\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "dL/dw: Variable containing:\n",
      "-0.3601 -0.0032  0.4757\n",
      " 0.0399  0.4923 -0.6611\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "dL/db: Variable containing:\n",
      " 0.3013\n",
      "-0.7033\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "---learnable params---\n",
      "params: 2\n",
      "first param size torch.Size([2, 3])\n",
      "---by hand---\n",
      "Variable containing:\n",
      "-0.4044  0.1045  0.0144\n",
      " 0.4511  0.4602 -0.4097\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Variable containing:\n",
      " 0.2311\n",
      " 0.2148\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "---optimizer step---\n",
      "w: Parameter containing:\n",
      "-0.4044  0.1045  0.0144\n",
      " 0.4511  0.4602 -0.4097\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "b: Parameter containing:\n",
      " 0.2311\n",
      " 0.2148\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 3unitから2unitのlinear層(全結合層)\n",
    "linear = nn.Linear(3, 2)\n",
    "\n",
    "print('w:', linear.weight)\n",
    "print('b:', linear.bias)\n",
    "\n",
    "# 誤差関数(損失関数)：二乗誤差\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 最適化アルゴリズム：勾配降下法(SGD)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# バッチ5，入力3，出力2\n",
    "input = Variable(torch.randn(5, 3)) # 入力\n",
    "target = Variable(torch.randn(5, 2)) # 教師\n",
    "output = linear(input) # 入力からの出力\n",
    "loss = criterion(output, target) # 誤差関数の値\n",
    "print('loss:', loss)\n",
    "\n",
    "# backpropagation\n",
    "loss.backward()\n",
    "\n",
    "print('dL/dw:', linear.weight.grad)\n",
    "print('dL/db:', linear.bias.grad)\n",
    "\n",
    "# 学習可能なモデルのパラメータはlinear.parameters()のよって返される\n",
    "print('---learnable params---')\n",
    "params = list(linear.parameters())\n",
    "print('params:', len(params))\n",
    "print('first param size', params[0].size())\n",
    "\n",
    "print('\\n---by hand---')\n",
    "learning_rate = 0.01\n",
    "print(linear.weight.sub(linear.weight.grad * learning_rate))\n",
    "print(linear.bias.sub(linear.bias.grad * learning_rate))\n",
    "\n",
    "optimizer.step()    # Does the update\n",
    "print('---optimizer step---')\n",
    "print('w:', linear.weight)\n",
    "print('b:', linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
